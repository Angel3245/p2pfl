{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3333e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /var/folders/vm/p9lv42ms7ml71cjvsj4bnbnm0000gn/T/tmpfgo0gw8l\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /var/folders/vm/p9lv42ms7ml71cjvsj4bnbnm0000gn/T/tmpfgo0gw8l/_remote_module_non_sriptable.py\n",
      "Train: 54000 Val:6000 Test:10000\n",
      "INFO:root:Nodo a la escucha en localhost 49820\n",
      "Train: 54000 Val:6000 Test:10000\n",
      "INFO:root:Nodo a la escucha en localhost 49821\n",
      "INFO:root:Conexión aceptada con ('localhost', 49821)\n",
      "INFO:root:Broadcasting start learning...\n",
      "INFO:root:Training...\n",
      "INFO:root:Training...\n",
      "INFO:root:Model added (1/2)\n",
      "INFO:root:Model added (1/2)\n",
      "INFO:root:Broadcasting model to all clients...\n",
      "INFO:root:Broadcasting model to all clients...\n",
      "INFO:root:Model added (2/2)\n",
      "INFO:root:Agregating models.\n",
      "[54000, 54000]\n",
      "INFO:root:Round 1 of 2 finished. (('localhost', 49821))\n",
      "INFO:root:Model added (2/2)\n",
      "INFO:root:Agregating models.\n",
      "[54000, 54000]\n",
      "INFO:root:Round 1 of 2 finished. (('localhost', 49820))\n",
      "INFO:root:Training...\n",
      "INFO:root:Model added (1/2)\n",
      "INFO:root:Broadcasting model to all clients...\n",
      "INFO:root:Training...\n",
      "INFO:root:Model added (1/2)\n",
      "INFO:root:Broadcasting model to all clients...\n",
      "INFO:root:Model added (2/2)\n",
      "INFO:root:Agregating models.\n",
      "[54000, 54000]\n",
      "INFO:root:Round 2 of 2 finished. (('localhost', 49821))\n",
      "INFO:root:Finish!!.\n",
      "INFO:root:Model added (2/2)\n",
      "INFO:root:Agregating models.\n",
      "[54000, 54000]\n",
      "INFO:root:Round 2 of 2 finished. (('localhost', 49820))\n",
      "INFO:root:Finish!!.\n",
      "INFO:root:Bajando el nodo, dejando de escuchar en localhost 49820\n",
      "DEBUG:root:Closed connection: ('localhost', 49820)\n",
      "DEBUG:root:Closed connection: ('localhost', 49821)\n",
      "INFO:root:Bajando el nodo, dejando de escuchar en localhost 49821\n"
     ]
    }
   ],
   "source": [
    "from p2pfl.node import Node\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "def test_convergence(x):\n",
    "    n,r = x\n",
    "\n",
    "    # Node Creation\n",
    "    nodes = []\n",
    "    for i in range(n):\n",
    "        node = Node(host=\"localhost\")\n",
    "        node.start()\n",
    "        nodes.append(node)\n",
    "\n",
    "    # Node Connection\n",
    "    for i in range(len(nodes)-1):\n",
    "        nodes[i+1].connect_to(nodes[i].host,nodes[i].port)\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    # Check if they are connected\n",
    "    for node in nodes:\n",
    "        assert len(node.neightboors) == n-1\n",
    "\n",
    "    # Start Learning\n",
    "    nodes[0].set_start_learning(rounds=r,epochs=0)\n",
    "\n",
    "    # Wait 4 results\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "        finish = True\n",
    "        for f in [node.round is None for node in nodes]:\n",
    "            finish = finish and f\n",
    "\n",
    "        if finish:\n",
    "            break\n",
    "\n",
    "\n",
    "    # Validamos Modelos obtenidos sean iguales\n",
    "    model = None\n",
    "    first = True\n",
    "    for node in nodes:\n",
    "        if first:\n",
    "            model = node.learner.get_parameters()\n",
    "            first = False\n",
    "        else:\n",
    "            for layer in model:\n",
    "                a = torch.round(model[layer], decimals=2)\n",
    "                b = torch.round(node.learner.get_parameters()[layer], decimals=2)\n",
    "                assert torch.eq(a, b).all()\n",
    "\n",
    "    # Cerrar\n",
    "    for node in nodes:\n",
    "        node.stop()\n",
    "        time.sleep(.1) #Esperar por la asincronía\n",
    "\n",
    "        \n",
    "test_convergence((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e00247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from p2pfl.const import HEARTBEAT_FREC, TIEMOUT\n",
    "from p2pfl.node import Node\n",
    "import time\n",
    "\n",
    "n1 = Node(\"localhost\")\n",
    "n2 = Node(\"localhost\")\n",
    "n1.start()\n",
    "n2.start()\n",
    "\n",
    "def test_interrupt_train():\n",
    "    n1 = Node(\"localhost\")\n",
    "    n1.start()\n",
    "    n1.set_start_learning(99999,99999)\n",
    "\n",
    "    time.sleep(1) #Esperar por la asincronía\n",
    "\n",
    "    n1.set_stop_learning()\n",
    "    n1.stop()\n",
    "    \n",
    "test_interrupt_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d7ffdf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "agregate() missing 1 required positional argument: 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtrunc(result[layer]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39meq(a, b)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtest_avg_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mtest_avg_complex\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     params1[layer] \u001b[38;5;241m=\u001b[39m params1[layer]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     21\u001b[0m     params2[layer] \u001b[38;5;241m=\u001b[39m params2[layer]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 23\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mFedAvg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams1\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams2\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check Results -> Careful with rounding errors\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m params:\n",
      "\u001b[0;31mTypeError\u001b[0m: agregate() missing 1 required positional argument: 'models'"
     ]
    }
   ],
   "source": [
    "from p2pfl.learning.agregators.fedavg import FedAvg\n",
    "from p2pfl.learning.pytorch.models.mlp import MLP\n",
    "from p2pfl.learning.pytorch.learners.lightninglearner import LightningLearner\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "def test_avg_complex():\n",
    "    agregator = FedAvg(None)\n",
    "    nl1 = LightningLearner(MLP(), None)\n",
    "    params = nl1.get_parameters()\n",
    "    params1 = nl1.get_parameters()\n",
    "    params2 = nl1.get_parameters()\n",
    "\n",
    "    result = agregator.agregate([(params,1)])\n",
    "\n",
    "    # Check Results\n",
    "    for layer in params:\n",
    "        assert torch.eq(params[layer], result[layer]).all()\n",
    "\n",
    "    for layer in params2:\n",
    "        params1[layer] = params1[layer]+1\n",
    "        params2[layer] = params2[layer]-1\n",
    "    \n",
    "    result = FedAvg.agregate([(params1,1), (params2,1)])\n",
    "\n",
    "    # Check Results -> Careful with rounding errors\n",
    "    for layer in params:\n",
    "        a = torch.trunc(params[layer]*10)\n",
    "        b = torch.trunc(result[layer]*10)\n",
    "        assert torch.eq(a, b).all()\n",
    "\n",
    "test_avg_complex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f35e9842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.]), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "sum = [torch.zeros_like(sum[layer]) for layer in sum]\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fc66a1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from p2pfl.learning.agregators.fedavg import FedAvg\n",
    "from p2pfl.learning.pytorch.models.mlp import MLP\n",
    "from p2pfl.learning.pytorch.learners.lightninglearner import LightningLearner\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def test_avg_simple():\n",
    "\n",
    "    a = OrderedDict([('a', torch.tensor(-1)), ('b', torch.tensor(-1))])\n",
    "    b = OrderedDict([('a', torch.tensor(0)), ('b', torch.tensor(0))])\n",
    "    c = OrderedDict([('a', torch.tensor(1)), ('b', torch.tensor(1))])\n",
    "\n",
    "    result = FedAvg.agregate([(a,1),(b,1),(c,1)])\n",
    "\n",
    "    for layer in b:\n",
    "        assert result[layer] == b[layer]\n",
    "        \n",
    "test_avg_simple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee5cd38",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39meq(a, b)\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mtest_avg_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mtest_avg_complex\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     params1[layer] \u001b[38;5;241m=\u001b[39m params1[layer]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     22\u001b[0m     params2[layer] \u001b[38;5;241m=\u001b[39m params2[layer]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 24\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mFedAvg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams1\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams2\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Check Results\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m params:\n",
      "File \u001b[0;32m~/Documents/UDC/4/TFG/federated_learning_p2p/p2pfl/learning/agregators/fedavg.py:50\u001b[0m, in \u001b[0;36mFedAvg.agregate\u001b[0;34m(models)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m,w \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m m:\n\u001b[0;32m---> 50\u001b[0m         accum[layer] \u001b[38;5;241m=\u001b[39m \u001b[43maccum\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mw\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Normalize Accum\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m accum:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from p2pfl.learning.agregators.fedavg import FedAvg\n",
    "from p2pfl.learning.pytorch.models.mlp import MLP\n",
    "from p2pfl.learning.pytorch.learners.lightninglearner import LightningLearner\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "\n",
    "\n",
    "def test_avg_complex():\n",
    "    nl1 = LightningLearner(MLP(), None)\n",
    "    params = nl1.get_parameters()\n",
    "    params1 = nl1.get_parameters()\n",
    "    params2 = nl1.get_parameters()\n",
    "\n",
    "    result = FedAvg.agregate([(params,1)])\n",
    "\n",
    "    # Check Results\n",
    "    for layer in params:\n",
    "        assert torch.eq(params[layer], result[layer]).all()\n",
    "\n",
    "    for layer in params2:\n",
    "        params1[layer] = params1[layer]+1\n",
    "        params2[layer] = params2[layer]-1\n",
    "    \n",
    "    result = FedAvg.agregate([(params1,1), (params2,1)])\n",
    "\n",
    "    # Check Results\n",
    "    for layer in params:\n",
    "        a = torch.trunc(params[layer]*10)\n",
    "        b = torch.trunc(result[layer]*10)\n",
    "        assert torch.eq(a, b).all()\n",
    "        \n",
    "while True:\n",
    "    test_avg_complex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "758ef836",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2964970389.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [3]\u001b[0;36m\u001b[0m\n\u001b[0;31m    1.shape()\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c81d0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from p2pfl.node import Node\n",
    "\n",
    "\n",
    "n1 = Node(\"localhost\")\n",
    "n1.start()\n",
    "\n",
    "n1.learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2faf4185",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43m[\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'all'"
     ]
    }
   ],
   "source": [
    "[True].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5af9785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'c']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myFun(a,*argv): \n",
    "    for arg in argv: \n",
    "        print (arg)\n",
    "\n",
    "[myFun(cmd,args) for cmd,args in [(\"a\",[\"b\",\"c\"])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a8c17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

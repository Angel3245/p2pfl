#
# This file is part of the federated_learning_p2p (p2pfl) distribution (see https://github.com/pguijas/federated_learning_p2p).
# Copyright (c) 2022 Pedro Guijas Bravo.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, version 3.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.
#

from test.utils import (
    set_test_settings,
    wait_4_results,
)
import numpy as np

set_test_settings()
from p2pfl.learning.pytorch.mnist_examples.mnistfederated_dm import MnistFederatedDM
from p2pfl.learning.pytorch.mnist_examples.models.mlp import MLP
from p2pfl.learning.pytorch.mnist_examples.models.cnn import CNN
from p2pfl.node import Node
import time
from pytorch_lightning import Trainer
from p2pfl.learning.pytorch.logger import FederatedLogger


def wait_convergence(nodes, n_neis, wait=5, only_direct=False):
    acum = 0
    while True:
        begin = time.time()
        print([len(n.get_neighbors(only_direct=only_direct)) for n in nodes])
        if all(
            [len(n.get_neighbors(only_direct=only_direct)) == n_neis for n in nodes]
        ):
            break
        time.sleep(0.1)
        acum += time.time() - begin
        if acum > wait:
            assert False


def check_equal_models(nodes):
    model = None
    first = True
    for node in nodes:
        if first:
            model = node.learner.get_parameters()
            first = False
        else:
            # compare layers with a tolerance
            for layer in model:
                assert np.allclose(
                    model[layer], node.learner.get_parameters()[layer], atol=1e-5
                )


def test_convergence(n, r):
    # Node Creation
    nodes = []
    for i in range(n):
        node = Node(MLP(), MnistFederatedDM())
        node.start()
        nodes.append(node)

    # Node Connection
    for i in range(len(nodes) - 1):
        nodes[i + 1].connect(nodes[i].addr)
        time.sleep(0.1)
    wait_convergence(nodes, n - 1, only_direct=False)

    # Start Learning
    nodes[0].set_start_learning(rounds=r, epochs=2)

    # Wait and check
    wait_4_results(nodes)
    check_equal_models(nodes)

    # Get logs
    print(nodes[0].learner.get_logs())

    # Stop Nodes
    [n.stop() for n in nodes]


if __name__ == "__main__":
    test_convergence(2, 2)
    """
    import matplotlib.pyplot as plt
    log = [{'127.0.0.1:54158': {'test_loss': [(0, 2.3027305603027344), (1, 0.10383020341396332), (2, 0.06538432091474533)], 'test_metric': [(0, 0.09730000048875809), (1, 0.9675999879837036), (2, 0.9812999963760376)], 'fl_round': [(49, 0), (99, 0), (149, 0), (199, 0), (249, 0), (299, 0), (349, 0), (399, 0), (449, 0), (499, 0), (549, 0), (599, 0), (649, 0), (699, 0), (749, 0), (799, 0), (849, 0), (899, 0), (949, 0), (999, 0), (1049, 0), (1099, 0), (1149, 0), (1199, 0), (1249, 0), (1299, 0), (1349, 0), (1399, 0), (1449, 0), (1499, 0), (1549, 0), (1599, 0), (1649, 0), (1687, 0), (1699, 0), (1749, 0), (1799, 0), (1849, 0), (1899, 0), (1949, 0), (1999, 0), (2049, 0), (2099, 0), (2149, 0), (2199, 0), (2249, 0), (2299, 0), (2349, 0), (2399, 0), (2449, 0), (2499, 0), (2549, 0), (2599, 0), (2649, 0), (2699, 0), (2749, 0), (2799, 0), (2849, 0), (2899, 0), (2949, 0), (2999, 0), (3049, 0), (3099, 0), (3149, 0), (3199, 0), (3249, 0), (3299, 0), (3349, 0), (3375, 0), (3424, 1), (3474, 1), (3524, 1), (3574, 1), (3624, 1), (3674, 1), (3724, 1), (3774, 1), (3824, 1), (3874, 1), (3924, 1), (3974, 1), (4024, 1), (4074, 1), (4124, 1), (4174, 1), (4224, 1), (4274, 1), (4324, 1), (4374, 1), (4424, 1), (4474, 1), (4524, 1), (4574, 1), (4624, 1), (4674, 1), (4724, 1), (4774, 1), (4824, 1), (4874, 1), (4924, 1), (4974, 1), (5024, 1), (5062, 1), (5074, 1), (5124, 1), (5174, 1), (5224, 1), (5274, 1), (5324, 1), (5374, 1), (5424, 1), (5474, 1), (5524, 1), (5574, 1), (5624, 1), (5674, 1), (5724, 1), (5774, 1), (5824, 1), (5874, 1), (5924, 1), (5974, 1), (6024, 1), (6074, 1), (6124, 1), (6174, 1), (6224, 1), (6274, 1), (6324, 1), (6374, 1), (6424, 1), (6474, 1), (6524, 1), (6574, 1), (6624, 1), (6674, 1), (6724, 1), (6750, 1)], 'train_loss': [(49, 0.48103517293930054), (99, 0.36745572090148926), (149, 0.28135642409324646), (199, 0.4577289819717407), (249, 0.24245338141918182), (299, 0.17032919824123383), (349, 0.36810484528541565), (399, 0.2549935281276703), (449, 0.3290155827999115), (499, 0.053933002054691315), (549, 0.23180803656578064), (599, 0.09755069017410278), (649, 0.15129093825817108), (699, 0.10110193490982056), (749, 0.14396019279956818), (799, 0.15627476572990417), (849, 0.29427456855773926), (899, 0.20339493453502655), (949, 0.3858747184276581), (999, 0.06640265136957169), (1049, 0.2281978279352188), (1099, 0.09384533762931824), (1149, 0.084227055311203), (1199, 0.1405266523361206), (1249, 0.0994160994887352), (1299, 0.23938420414924622), (1349, 0.11296074092388153), (1399, 0.09753073751926422), (1449, 0.3303997814655304), (1499, 0.033540837466716766), (1549, 0.017536401748657227), (1599, 0.04730653017759323), (1649, 0.538288950920105), (1699, 0.04610956832766533), (1749, 0.2135668247938156), (1799, 0.05098140612244606), (1849, 0.06531711667776108), (1899, 0.0787077471613884), (1949, 0.1599939614534378), (1999, 0.06286709755659103), (2049, 0.09979434311389923), (2099, 0.03738050162792206), (2149, 0.018485847860574722), (2199, 0.04522349685430527), (2249, 0.37758806347846985), (2299, 0.02474515326321125), (2349, 0.10316900908946991), (2399, 0.017870016396045685), (2449, 0.06558291614055634), (2499, 0.2641454339027405), (2549, 0.12526388466358185), (2599, 0.023944266140460968), (2649, 0.2007126361131668), (2699, 0.05331059917807579), (2749, 0.2515258193016052), (2799, 0.07790140807628632), (2849, 0.03887777775526047), (2899, 0.032151997089385986), (2949, 0.05227646976709366), (2999, 0.059760887175798416), (3049, 0.25094521045684814), (3099, 0.1156967431306839), (3149, 0.027370024472475052), (3199, 0.04256289452314377), (3249, 0.08388590812683105), (3299, 0.0872681736946106), (3349, 0.10660985112190247), (3424, 0.13486339151859283), (3474, 0.04099636897444725), (3524, 0.036641400307416916), (3574, 0.15362147986888885), (3624, 0.021718688309192657), (3674, 0.05342017114162445), (3724, 0.11493801325559616), (3774, 0.06977283209562302), (3824, 0.02665029466152191), (3874, 0.02213037945330143), (3924, 0.022629713639616966), (3974, 0.00399933522567153), (4024, 0.014402435161173344), (4074, 0.41624361276626587), (4124, 0.0325014628469944), (4174, 0.05381859466433525), (4224, 0.015831368044018745), (4274, 0.031252119690179825), (4324, 0.0033499416895210743), (4374, 0.2647012174129486), (4424, 0.027895504608750343), (4474, 0.021619221195578575), (4524, 0.08152525126934052), (4574, 0.10485657304525375), (4624, 0.08896571397781372), (4674, 0.017867133021354675), (4724, 0.2060714066028595), (4774, 0.10212820768356323), (4824, 0.0197683647274971), (4874, 0.022321704775094986), (4924, 0.059598010033369064), (4974, 0.02546100877225399), (5024, 0.00417268555611372), (5074, 0.11712239682674408), (5124, 0.0065992046147584915), (5174, 0.01761806756258011), (5224, 0.008381595835089684), (5274, 0.09090134501457214), (5324, 0.009955310262739658), (5374, 0.028075210750102997), (5424, 0.09230055660009384), (5474, 0.07384122163057327), (5524, 0.018728071823716164), (5574, 0.08175718784332275), (5624, 0.05647772178053856), (5674, 0.029284857213497162), (5724, 0.12786605954170227), (5774, 0.008371707051992416), (5824, 0.004916511010378599), (5874, 0.07462601363658905), (5924, 0.2130793333053589), (5974, 0.0024206414818763733), (6024, 0.15468569099903107), (6074, 0.02286093309521675), (6124, 0.4318773150444031), (6174, 0.004678945988416672), (6224, 0.0022069616243243217), (6274, 0.12685906887054443), (6324, 0.19688622653484344), (6374, 0.07868877798318863), (6424, 0.0028862582985311747), (6474, 0.0017078750533983111), (6524, 0.02988479845225811), (6574, 0.0034066704101860523), (6624, 0.07174969464540482), (6674, 0.004959996789693832), (6724, 0.03266490250825882)], 'epoch': [(49, 0), (99, 0), (149, 0), (199, 0), (249, 0), (299, 0), (349, 0), (399, 0), (449, 0), (499, 0), (549, 0), (599, 0), (649, 0), (699, 0), (749, 0), (799, 0), (849, 0), (899, 0), (949, 0), (999, 0), (1049, 0), (1099, 0), (1149, 0), (1199, 0), (1249, 0), (1299, 0), (1349, 0), (1399, 0), (1449, 0), (1499, 0), (1549, 0), (1599, 0), (1649, 0), (1687, 0), (1699, 1), (1749, 1), (1799, 1), (1849, 1), (1899, 1), (1949, 1), (1999, 1), (2049, 1), (2099, 1), (2149, 1), (2199, 1), (2249, 1), (2299, 1), (2349, 1), (2399, 1), (2449, 1), (2499, 1), (2549, 1), (2599, 1), (2649, 1), (2699, 1), (2749, 1), (2799, 1), (2849, 1), (2899, 1), (2949, 1), (2999, 1), (3049, 1), (3099, 1), (3149, 1), (3199, 1), (3249, 1), (3299, 1), (3349, 1), (3375, 1), (3424, 0), (3474, 0), (3524, 0), (3574, 0), (3624, 0), (3674, 0), (3724, 0), (3774, 0), (3824, 0), (3874, 0), (3924, 0), (3974, 0), (4024, 0), (4074, 0), (4124, 0), (4174, 0), (4224, 0), (4274, 0), (4324, 0), (4374, 0), (4424, 0), (4474, 0), (4524, 0), (4574, 0), (4624, 0), (4674, 0), (4724, 0), (4774, 0), (4824, 0), (4874, 0), (4924, 0), (4974, 0), (5024, 0), (5062, 0), (5074, 1), (5124, 1), (5174, 1), (5224, 1), (5274, 1), (5324, 1), (5374, 1), (5424, 1), (5474, 1), (5524, 1), (5574, 1), (5624, 1), (5674, 1), (5724, 1), (5774, 1), (5824, 1), (5874, 1), (5924, 1), (5974, 1), (6024, 1), (6074, 1), (6124, 1), (6174, 1), (6224, 1), (6274, 1), (6324, 1), (6374, 1), (6424, 1), (6474, 1), (6524, 1), (6574, 1), (6624, 1), (6674, 1), (6724, 1), (6750, 1)], 'val_loss': [(1687, 0.12210754305124283), (3375, 0.10445462912321091), (5062, 0.0849703997373581), (6750, 0.08482665568590164)], 'val_metric': [(1687, 0.9649999737739563), (3375, 0.9695000052452087), (5062, 0.9754999876022339), (6750, 0.9783333539962769)]}, '127.0.0.1:54160': {'test_loss': [(0, 2.3027305603027344), (1, 0.10383020341396332), (2, 0.06538432091474533)], 'test_metric': [(0, '0.09730000048875809'), (1, '0.9675999879837036'), (2, '0.9812999963760376')]}}]
    for _, node in log[0].items():
        for metric, values in node.items():
            print(v)
            v = zip(*values)
            plt.plot(*v)
            plt.title(metric)
            plt.show()
    """
